## 论文整理

### 深度学习







### 目标检测

+ YOLOv1
+ YOLOv2
+ YOLOv3





### 基于深度学习的深度估计

#### Part 1 前期基础论文

+ 1. 2014 NIPS [Depth Map Prediction from a Single Image using a Multi-Scale Deep Network](http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1406.2283.pdf) 第一篇CNN-based来做的单目估计深度的论文

这里的Multi-scale不是现在网络中Multi-scale features的做法，而是分为两个scale的网络来做DepthMap的估计，分别是Global Coarse-Scale Network和Local Fine-Scale Network。前者其实就是AlexNet，来得到一个低分辨率的Coarse的Depth Map,再用后者去refine前者的输出得到最后的refined depth map

代码：<https://github.com/hjimce/Depth-Map-Prediction>

+ 2. 2015 ICCV 也是上一篇的作者David Eigen发的 [Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture](https://arxiv.org/pdf/1411.4734.pdf)　

可以预测出Depth, Normals, Labels

+ 3. 2016 IEEE 3D Vision [Deeper Depth Prediction with Fully Convolutional Residual Networks](https://arxiv.org/pdf/1606.00373.pdf) FCRN

这是一个比较经典的用神经网络估计深度的算法

代码：<https://github.com/iro-cp/FCRN-DepthPrediction>　

+ 4. 2017 [A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images](https://arxiv.org/pdf/1607.00730.pdf)



#### Part 2  
自己看过跑过代码效果比较好，可以借鉴的论文

+ 1. 2018 Arxiv DenseDepthNet [Dense Depth:High Quality Monocular Depth Estumation via Transfer Learning](https://arxiv.org/pdf/1812.11941.pdf)

这篇论文在室内场景的测试效果很好，边缘清晰泛化好，生成的点云效果也很好。主要就是迁移学习和Loss两部分，由于不知道迁移学习那部分怎么进行耦合，所以只能在后面生成深度的网络中训练。没有使用几何信息。

代码：https://github.com/ialhashim/DenseDepth

+ 2. 2018 MIT [Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image](https://arxiv.org/pdf/1709.07492.pdf)

加上稀疏点约束计算深度


代码：https://github.com/fangchangma/sparse-to-dense.pytorch

+ 3. 2018 HKUST [MVDepthNet: Real-time Multiview Depth Estimation Neural Network](https://arxiv.org/abs/1807.08563)

输入reference image 和一序列source images 计算cost volume(采用DTAM中关于cost volume 的构建方式)　几何和深度学习的图像feature是分离的

笔记：[MVDepthNet](./MVDepthNet/MVDepthNet.md)

+ 4. 2018 HKUST ECCV [MVSNet: Depth Inference for Unstructured Multi-view Stereo](https://arxiv.org/abs/1804.02505)

还是港科大做的，这个算法将几何信息和神经网络输出耦合在一起，将神经网络得到的特征图计算homogarphy，由此构造一个cost volume　

笔记：[MVSNet](./MVSNet/MVSNet.md)

<https://blog.csdn.net/john_xia/article/details/88100410>



+ 5. 2019 HKUST [Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference](<https://arxiv.org/abs/1902.10556>) 

解决了上一版本训练时的内存消耗问题，添加GRU 单元降低内存消耗，同时提高了输出深度图的分辨率。

代码：https://github.com/YoYo000/MVSNet







